{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7bJQCiYae6m"
      },
      "source": [
        "# Implémentation de l’Architecture GPT-2 avec NumPy\n",
        "\n",
        "## Construction d’un modèle de langage auto-régressif (décodeur-uniquement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HET8CQa2aNT9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np5ZPCeIbw8e"
      },
      "source": [
        "#Part I : Empécher de Voir le Futur: Le Masque Causal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRRZxEHb-HW"
      },
      "source": [
        "##Problem 1: Implémentation du Masque Causal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqoa8eFQcRsm"
      },
      "source": [
        "###a) Créez une fonction Python create_causal_mask(seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvkyMkzebwkP"
      },
      "outputs": [],
      "source": [
        "def create_causal_mask(seq_len):\n",
        "  #b) retourne une matrice de taille (seq_len, seq_len)\n",
        "  #c) retourne 1 pour le triangle en dessous de la diagonale et 0 pour le triangle au dessus\n",
        "  #d) Astuce fonction np.tril()\n",
        "  return np.tril(np.ones((seq_len, seq_len)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "DKa3k1fHcsOX",
        "outputId": "2fb96c36-4672-4363-f652-b71d8dd9a8b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] \n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAGdCAYAAAAv9mXmAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAE9pJREFUeJzt3XGo1YX9//G33ub1ru+9F7NpXrqmxb5YapldkxTaRpJExfoy2gIDMRhju6UmxHSjJJzeHFsI2ixla8KyDIbU4lcSd6RzJZp2I9mmG/Frl0QtkHvM4ObuPb8/9t39fv1ZzqP37eec6+MBnz/uh3M8L05xn3zuufecYeVyuRwAMMiGFz0AgKFJYABIITAApBAYAFIIDAApBAaAFAIDQAqBASDFJRf6Afv7++PQoUPR2NgYw4YNu9APD8B5KJfLcfz48WhpaYnhw898jXLBA3Po0KFobW290A8LwCDq7u6OK6+88oy3ueCBaWxsjIiID/ZNiKb/qJ6f0P3Xf04tegJA1ftHnIyd8X8GvpefyQUPzL9+LNb0H8OjqbF6AnPJsC8VPQGg+v33u1eezUsc1fMdHoAhRWAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkOKfAPPXUUzFhwoQYOXJkzJw5M3bv3j3YuwCocRUHZsuWLbFkyZJYvnx57Nu3L2644YaYO3duHD16NGMfADWq4sA8+eST8d3vfjcWLFgQ1113XTz99NPx5S9/OX71q19l7AOgRlUUmM8++yz27t0bc+bM+Z9/YPjwmDNnTrz11lufe5/e3t4olUqnHAAMfRUF5uOPP46+vr4YO3bsKefHjh0bhw8f/tz7dHR0RHNz88Dh0ywBLg7pv0W2bNmy6OnpGTi6u7uzHxKAKlDRJ1pefvnlUVdXF0eOHDnl/JEjR+KKK6743PvU19dHfX39uS8EoCZVdAUzYsSIuOmmm6Kzs3PgXH9/f3R2dsYtt9wy6OMAqF0VXcFERCxZsiTmz58fbW1tcfPNN8eaNWvixIkTsWDBgox9ANSoigPzne98Jz766KN47LHH4vDhwzFt2rR47bXXTnvhH4CL27ByuVy+kA9YKpWiubk5jh28Opoaq+edaua2TCt6AkDV+0f5ZLwRL0VPT080NTWd8bbV8x0egCFFYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKSo+M0uh6pth7qKnnAa748G1DJXMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFJcUPYAvtu1QV9ETTjO3ZVrRE4Aa4QoGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApKgoMB0dHTFjxoxobGyMMWPGxD333BMHDhzI2gZADasoMNu3b4/29vbYtWtXvP7663Hy5Mm4/fbb48SJE1n7AKhRFX3g2GuvvXbK17/+9a9jzJgxsXfv3rj11lsHdRgAte28PtGyp6cnIiIuu+yyL7xNb29v9Pb2DnxdKpXO5yEBqBHn/CJ/f39/LF68OGbPnh1Tpkz5wtt1dHREc3PzwNHa2nquDwlADTnnwLS3t8f+/fvjhRdeOOPtli1bFj09PQNHd3f3uT4kADXknH5E9uCDD8Yrr7wSO3bsiCuvvPKMt62vr4/6+vpzGgdA7aooMOVyOR566KHYunVrvPHGGzFx4sSsXQDUuIoC097eHps3b46XXnopGhsb4/DhwxER0dzcHA0NDSkDAahNFb0Gs379+ujp6Ymvf/3rMW7cuIFjy5YtWfsAqFEV/4gMAM6G9yIDIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASHFeH5nMxWfboa6iJ5xmbsu0oicAn8MVDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEgxSVFD4Dzte1QV9ETTjO3ZVrRE6BwrmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNAivMKzBNPPBHDhg2LxYsXD9IcAIaKcw7Mnj174plnnonrr79+MPcAMEScU2A++eSTmDdvXmzcuDFGjRo12JsAGALOKTDt7e1x5513xpw5c/7tbXt7e6NUKp1yADD0VfyRyS+88ELs27cv9uzZc1a37+joiMcff7ziYQDUtoquYLq7u2PRokXx3HPPxciRI8/qPsuWLYuenp6Bo7u7+5yGAlBbKrqC2bt3bxw9ejSmT58+cK6vry927NgR69ati97e3qirqzvlPvX19VFfXz84awGoGRUF5rbbbov33nvvlHMLFiyISZMmxQ9/+MPT4gLAxauiwDQ2NsaUKVNOOXfppZfG6NGjTzsPwMXNX/IDkKLi3yL7/73xxhuDMAOAocYVDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0CK834vMuB02w51FT3hNHNbphU9gYuMKxgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQIpLih4AXBjbDnUVPeE0c1umFT2BRK5gAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQIqKA/Phhx/G/fffH6NHj46GhoaYOnVqvP322xnbAKhhFX0ezLFjx2L27NnxjW98I1599dX4yle+En/9619j1KhRWfsAqFEVBWb16tXR2toazz777MC5iRMnDvooAGpfRT8ie/nll6OtrS3uvffeGDNmTNx4442xcePGM96nt7c3SqXSKQcAQ19FgXn//fdj/fr18dWvfjW2bdsW3//+92PhwoWxadOmL7xPR0dHNDc3Dxytra3nPRqA6jesXC6Xz/bGI0aMiLa2tnjzzTcHzi1cuDD27NkTb7311ufep7e3N3p7ewe+LpVK0draGscOXh1NjX6JDS5mc1umFT2BCv2jfDLeiJeip6cnmpqaznjbir7Djxs3Lq677rpTzl177bXx97///QvvU19fH01NTaccAAx9FQVm9uzZceDAgVPOHTx4MK666qpBHQVA7asoMA8//HDs2rUrVq1aFX/7299i8+bNsWHDhmhvb8/aB0CNqigwM2bMiK1bt8bzzz8fU6ZMiRUrVsSaNWti3rx5WfsAqFEV/R1MRMRdd90Vd911V8YWAIYQv8YFQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkKLi9yIDGCzbDnUVPeE0PgRt8LiCASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkuKToAQDVZNuhrqInnGZuy7SiJ5wTVzAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEgRUWB6evri0cffTQmTpwYDQ0Ncc0118SKFSuiXC5n7QOgRlX0eTCrV6+O9evXx6ZNm2Ly5Mnx9ttvx4IFC6K5uTkWLlyYtRGAGlRRYN5888345je/GXfeeWdEREyYMCGef/752L17d8o4AGpXRT8imzVrVnR2dsbBgwcjIuLdd9+NnTt3xh133PGF9+nt7Y1SqXTKAcDQV9EVzNKlS6NUKsWkSZOirq4u+vr6YuXKlTFv3rwvvE9HR0c8/vjj5z0UgNpS0RXMiy++GM8991xs3rw59u3bF5s2bYqf/exnsWnTpi+8z7Jly6Knp2fg6O7uPu/RAFS/iq5gHnnkkVi6dGncd999ERExderU+OCDD6KjoyPmz5//ufepr6+P+vr6818KQE2p6Arm008/jeHDT71LXV1d9Pf3D+ooAGpfRVcwd999d6xcuTLGjx8fkydPjnfeeSeefPLJeOCBB7L2AVCjKgrM2rVr49FHH40f/OAHcfTo0WhpaYnvfe978dhjj2XtA6BGDStf4D/DL5VK0dzcHMcOXh1Njd6pBuDfmdsyregJA/5RPhlvxEvR09MTTU1NZ7yt7/AApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKSp6s0sALrxth7qKnjCgdLw/Rv3n2d3WFQwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNAiksu9AOWy+WIiCh90n+hHxqA8/Sv793/+l5+Jhc8MMePH4+IiKum/98L/dAADJLjx49Hc3PzGW8zrHw2GRpE/f39cejQoWhsbIxhw4ad879TKpWitbU1uru7o6mpaRAXDi2ep7PjeTo7nqezM5Sfp3K5HMePH4+WlpYYPvzMr7Jc8CuY4cOHx5VXXjlo/15TU9OQ+w+YwfN0djxPZ8fzdHaG6vP0765c/sWL/ACkEBgAUtRsYOrr62P58uVRX19f9JSq5nk6O56ns+N5Ojuep3+64C/yA3BxqNkrGACqm8AAkEJgAEghMACkqNnAPPXUUzFhwoQYOXJkzJw5M3bv3l30pKrS0dERM2bMiMbGxhgzZkzcc889ceDAgaJnVbUnnngihg0bFosXLy56StX58MMP4/7774/Ro0dHQ0NDTJ06Nd5+++2iZ1WVvr6+ePTRR2PixInR0NAQ11xzTaxYseKs3rNrqKrJwGzZsiWWLFkSy5cvj3379sUNN9wQc+fOjaNHjxY9rWps37492tvbY9euXfH666/HyZMn4/bbb48TJ04UPa0q7dmzJ5555pm4/vrri55SdY4dOxazZ8+OL33pS/Hqq6/Gn/70p/j5z38eo0aNKnpaVVm9enWsX78+1q1bF3/+859j9erV8dOf/jTWrl1b9LTC1OSvKc+cOTNmzJgR69ati4h/vr9Za2trPPTQQ7F06dKC11Wnjz76KMaMGRPbt2+PW2+9teg5VeWTTz6J6dOnxy9+8Yv4yU9+EtOmTYs1a9YUPatqLF26NP74xz/GH/7wh6KnVLW77rorxo4dG7/85S8Hzn3rW9+KhoaG+M1vflPgsuLU3BXMZ599Fnv37o05c+YMnBs+fHjMmTMn3nrrrQKXVbeenp6IiLjssssKXlJ92tvb48477zzl/yn+x8svvxxtbW1x7733xpgxY+LGG2+MjRs3Fj2r6syaNSs6Ozvj4MGDERHx7rvvxs6dO+OOO+4oeFlxLvibXZ6vjz/+OPr6+mLs2LGnnB87dmz85S9/KWhVdevv74/FixfH7NmzY8qUKUXPqSovvPBC7Nu3L/bs2VP0lKr1/vvvx/r162PJkiXxox/9KPbs2RMLFy6MESNGxPz584ueVzWWLl0apVIpJk2aFHV1ddHX1xcrV66MefPmFT2tMDUXGCrX3t4e+/fvj507dxY9pap0d3fHokWL4vXXX4+RI0cWPadq9ff3R1tbW6xatSoiIm688cbYv39/PP300wLzv7z44ovx3HPPxebNm2Py5MnR1dUVixcvjpaWlov2eaq5wFx++eVRV1cXR44cOeX8kSNH4oorrihoVfV68MEH45VXXokdO3YM6sckDAV79+6No0ePxvTp0wfO9fX1xY4dO2LdunXR29sbdXV1BS6sDuPGjYvrrrvulHPXXntt/Pa3vy1oUXV65JFHYunSpXHfffdFRMTUqVPjgw8+iI6Ojos2MDX3GsyIESPipptuis7OzoFz/f390dnZGbfcckuBy6pLuVyOBx98MLZu3Rq///3vY+LEiUVPqjq33XZbvPfee9HV1TVwtLW1xbx586Krq0tc/tvs2bNP+xX3gwcPxlVXXVXQour06aefnvYBXHV1ddHff/F+PHzNXcFERCxZsiTmz58fbW1tcfPNN8eaNWvixIkTsWDBgqKnVY329vbYvHlzvPTSS9HY2BiHDx+OiH9+UFBDQ0PB66pDY2Pjaa9JXXrppTF69GivVf0vDz/8cMyaNStWrVoV3/72t2P37t2xYcOG2LBhQ9HTqsrdd98dK1eujPHjx8fkyZPjnXfeiSeffDIeeOCBoqcVp1yj1q5dWx4/fnx5xIgR5Ztvvrm8a9euoidVlYj43OPZZ58telpV+9rXvlZetGhR0TOqzu9+97vylClTyvX19eVJkyaVN2zYUPSkqlMqlcqLFi0qjx8/vjxy5Mjy1VdfXf7xj39c7u3tLXpaYWry72AAqH419xoMALVBYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABS/D97Wl7STGp/SQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#e) Visualisation\n",
        "mask = create_causal_mask(10)\n",
        "print(mask,'\\n')\n",
        "\n",
        "plt.imshow(mask)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhqRoyideSnZ"
      },
      "source": [
        "#Part II La Brique Fondamentale: Le Bloc Décodeur de GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4hHc2RBfc8D"
      },
      "source": [
        "##Problem 2: Construction du Bloc Décodeur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKtkmgdafyi5"
      },
      "source": [
        "##a) Réutilisation : classes MultiHeadAttention, FeedForward, et LayerNorm du TP 2\n",
        "\n",
        "- Pour MultiHeadAttention, la seule adaptation se fera dans la mani`ere\n",
        "dont nous l’appelons (en lui passant syst´ematiquement un masque causal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1jtD3_ygM35"
      },
      "outputs": [],
      "source": [
        "def  scaled_dot_product_attention(Q, K, V,mask=None):\n",
        "  # b) calcul des score\n",
        "  score = Q @ K.swapaxes(-2, -1)\n",
        "\n",
        "  #c) mise à l'échelle\n",
        "  scaled_score = score/np.sqrt(Q.shape[-1])\n",
        "  ## elle permet de garder une variance de 1 et de ne pas saturer le softmax.\n",
        "\n",
        "  #d) Masquag\n",
        "  if mask is not None:\n",
        "    scaled_score = np.where(mask, scaled_score, -1e9)\n",
        "\n",
        "  #e) Softmax\n",
        "  exp_score = np.exp(scaled_score - np.max(scaled_score, axis=-1, keepdims=True))\n",
        "  attention_weights = exp_score / np.sum(exp_score, axis=-1, keepdims=True)\n",
        "\n",
        "  #f) sortie\n",
        "  Z = attention_weights @ V\n",
        "\n",
        "  #g) la fonction retourne Z et attention_qeights\n",
        "  return Z , attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUZSe7bggIFS"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention():\n",
        "\n",
        "  #b) Le constructeur __init__\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = n_heads\n",
        "    self.d_k =  d_model // n_heads\n",
        "    self.W_q = np.random.randn(d_model, d_model)\n",
        "    self.W_k = np.random.randn(d_model, d_model)\n",
        "    self.W_v = np.random.randn(d_model, d_model)\n",
        "    self.W_o = np.random.randn(d_model, d_model)\n",
        "\n",
        "    ### c) Création d'une méthode compute(self, Q,12, K, V, mask=None)\n",
        "  def compute(self, Q, K, V, mask=None):\n",
        "\n",
        "    #Projetion de Q, K, V en les multipliant par leurs matrices de poids respectives.\n",
        "    Q_projection = Q @ self.W_q\n",
        "    K_projection = K @ self.W_k\n",
        "    V_projection = V @ self.W_v\n",
        "\n",
        "    #Remodelez (np.reshape) les matrices Q, K, V a (batch size, n heads, seq len,d_k)\n",
        "    batch_size , seq_len , _ = Q.shape\n",
        "    Q_projection = Q_projection.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
        "    K_projection = K_projection.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
        "    V_projection = V_projection.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
        "\n",
        "\n",
        "    #Appel de la fonction scaled_dot_product_attention\n",
        "    Z_heads, attention_weights = scaled_dot_product_attention(Q_projection, K_projection, V_projection, mask)\n",
        "\n",
        "    #Concaténation des sorties des têtes\n",
        "    Z_concat = Z_heads.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    #prrojection finale\n",
        "    output = Z_concat @ self.W_o\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue7wCp4mgSgi"
      },
      "outputs": [],
      "source": [
        "class FeedForward():\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    self.W1 = np.random.randn(d_model, d_ff)\n",
        "    self.b1 = np.random.randn(d_ff)\n",
        "    self.W2 = np.random.randn(d_ff, d_model)\n",
        "    self.b2 = np.random.randn(d_model)\n",
        "\n",
        "  def compute(self, x):\n",
        "      FFN = np.maximum(0, x @ self.W1 + self.b1) @ self.W2 + self.b2\n",
        "      return FFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHkaDPYxgWAj"
      },
      "outputs": [],
      "source": [
        "class LayerNorm():\n",
        "  def __init__(self, d_model, epsilon=1e-5):\n",
        "    self.gamma = np.ones(d_model)\n",
        "    self.beta = np.zeros(d_model)\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def compute(self, x):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    var = np.var(x, axis=-1, keepdims=True)\n",
        "    return self.gamma * ((x - mean) / np.sqrt(var + self.epsilon)) + self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJLdKJqOhsjt"
      },
      "source": [
        "###b) Création de la classe Python DecoderBlock."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gucZj4o7hw58"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock():\n",
        "\n",
        "  #c) Le constructeur initialise une instance de MultiHeadAttention, de FeedForward, et de deux LayerNorm.\n",
        "  def __init__(self, d_model, n_heads, d_ff):\n",
        "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "    self.ffn = FeedForward(d_model, d_ff)\n",
        "    self.norm1 = LayerNorm(d_model)\n",
        "    self.norm2 = LayerNorm(d_model)\n",
        "\n",
        "  #d) Création d'une méthode compute(self, x, mask)\n",
        "  def compute(self, x, mask):\n",
        "    #i. Calcul de la sortie de la couche d’auto-attention multi-têtes (Masked Multi-Head  Self-Attention). passer le mask à cette sous-couche.\n",
        "    mha_output, attention_weights = self.mha.compute(x, x, x, mask)\n",
        "\n",
        "    #ii. Application de la première connexion résiduelle (Add) en ajoutant la sortie de l’attention à l’entrée x.\n",
        "    residu1 = x + mha_output\n",
        "\n",
        "    #iii. Application de la première normalisation de couche (Norm).*\n",
        "    norm1_output = self.norm1.compute(residu1)\n",
        "\n",
        "    #iv. passage du résultat dans le réseau Feed-Forward (FFN).\n",
        "    ffn_output = self.ffn.compute(norm1_output)\n",
        "\n",
        "    #v. Application de la seconde connexion résiduelle et la seconde normalisation.\n",
        "    residu2 = norm1_output + ffn_output\n",
        "    norm2_output = self.norm2.compute(residu2)\n",
        "\n",
        "    #vi. sortie finale du bloc.\n",
        "    return norm2_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtvzqJ36mTLn"
      },
      "source": [
        "###c) Le bloc décodeur de style GPT n’a  qu’une seule couche d’attention contrairement au bloc décodeur du Transformer original ”Attention Is All You Need” parce qu'il est unidirectionnel. C'est à dire qu'il est conçu pour générer le token suivant à partir du token précédent,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjhVcRCYnW5g"
      },
      "source": [
        "#Part III : Assemblage du Modèle GPT Complet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezWR-z-KnbPz"
      },
      "source": [
        "##Problem 3: Construction du Modèle GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-VyTbxwno9X"
      },
      "source": [
        "###a) Création d'une classe Python GPTModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGF5xdTCo4fJ"
      },
      "outputs": [],
      "source": [
        "def create_positional_encoding(max_len, d_model):\n",
        "  # création de la matrice PE de taile (max_len, d_model) de zeros\n",
        "  pe = np.zeros((max_len, d_model))\n",
        "\n",
        "  #création du vecteur de position et du terme de division\n",
        "  pos = np.arange(0, max_len).reshape(max_len, 1)\n",
        "  div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000) / d_model))\n",
        "  #implémentation du positionnal encoding\n",
        "  pe[:, 0::2] = np.sin(pos * div_term)\n",
        "  pe[:, 1::2] = np.cos(pos * div_term)\n",
        "  return pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5u4f5PAnsqJ"
      },
      "outputs": [],
      "source": [
        "class GPTModel():\n",
        "\n",
        "  #(b) Le constructeur prend en arguments vocab_size, d_model, n_layers, n_heads, d_ff, max_len\n",
        "  def __init__(self,vocab_size, d_model,n_layers , n_heads, d_ff, max_len):\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.n_heads = n_heads\n",
        "    self.d_ff = d_ff\n",
        "    self.max_len = max_len\n",
        "\n",
        "    #matrice d’embedding de tokens W_te de taille (vocab_size, d_model)\n",
        "    self.W_te = np.random.randn(vocab_size, d_model)\n",
        "\n",
        "    #matrice d’embedding de positions W_pe de taille (max_len, d_model)\n",
        "    self.W_pe = create_positional_encoding(max_len, d_model)\n",
        "\n",
        "    #liste de n_layers instances de votre DecoderBlock.\n",
        "    self.layers = [DecoderBlock(d_model, n_heads, d_ff) for i in range(n_layers)]\n",
        "\n",
        "    #Une couche de normalisation finale LayerNorm.\n",
        "    self.norm = LayerNorm(d_model)\n",
        "\n",
        "    # matrice de poids W_lm_head de taille (d model, vocab size)\n",
        "    self.W_lm_head = np.random.randn(d_model, vocab_size)\n",
        "\n",
        "\n",
        "  ####c) méthode compute(self, input ids).\n",
        "  def compute(self, input_ids):\n",
        "    #i. longueur de la séquence à partir de input_ids\n",
        "    batch_size, seq_len = input_ids.shape\n",
        "\n",
        "    #ii. Récupérez les embeddings de tokens à partir de W_te pour chaque ID dans input_ids\n",
        "    token_embeddings = self.W_te[input_ids]\n",
        "\n",
        "    #iii. Récupérez les embeddings de position à partir de W_pe pour les positions correspondantes.\n",
        "    position_indices = np.arange(seq_len)\n",
        "    position_embeddings = self.W_pe[position_indices]\n",
        "\n",
        "    #iv. Additionnez les deux embeddings pour obtenir le vecteur d’entrée x\n",
        "    x = token_embeddings + position_embeddings\n",
        "\n",
        "    #v. Créez le masque causal approprié pour la séquence en utilisant votre fonction create_causal_mask.\n",
        "    mask = create_causal_mask(seq_len)\n",
        "\n",
        "    #vi. Passez x et le masque à travers la pile de DecoderBlock séquentiellement.\n",
        "    for layer in self.layers:\n",
        "      x = layer.compute(x, mask)\n",
        "\n",
        "    #vii. Appliquez la normalisation de couche finale sur la sortie.\n",
        "    x = self.norm.compute(x)\n",
        "\n",
        "    #viii. Appliquez la projection finale de la tête de langage (@ W_lm_head) pour obtenir les logits.\n",
        "    logits = x @ self.W_lm_head\n",
        "\n",
        "    #ix. Retournez les logits.\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKDb--gvJ91"
      },
      "source": [
        "###(d) Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuw-q6bjvQbc",
        "outputId": "19763a0e-2f91-4790-b2c8-4d5155f68948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 10, 10000)\n"
          ]
        }
      ],
      "source": [
        "gptModel =  GPTModel(10000, 128, 4, 4, 2048, 100)\n",
        "input_ids = np.random.randint(0,1000,(1,10))\n",
        "logits = gptModel.compute(input_ids)\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JczqspZk14Nu"
      },
      "source": [
        "#Part IV Bonus: La Géenération de Texte en Pratique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LRHsLLm2Baj"
      },
      "source": [
        "##Problem 4: Implémentation de la Génération (Greedy Decoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kryGPevm2Obo"
      },
      "source": [
        "###a) Création d'une fonction generate(model, input_ids, max_new_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GbetkwS2Vaa"
      },
      "outputs": [],
      "source": [
        "def generate(model, input_ids, max_new_tokens):\n",
        "  #b) boucle qui s’exécute max_new_tokens fois\n",
        "  #c) A chaque itération de la boucle\n",
        "  for i in range(max_new_tokens):\n",
        "    #i. Passez la séquence input_ids actuelle au modèle pour obtenir les logits\n",
        "    logits = model.compute(input_ids)\n",
        "\n",
        "    #ii. logits de la dernière position\n",
        "    dernier_logits = logits[:, -1, :]\n",
        "\n",
        "    #iii. Appliquez la fonction np.argmax sur ces logits pour trouver l’ID du token le plus probable.\n",
        "    token_id = np.argmax(dernier_logits, axis=-1)\n",
        "\n",
        "    #iv. Ajoutez ce nouvel ID à la fin de votre séquence input_ids\n",
        "    input_ids = np.concatenate((input_ids, token_id.reshape(1, 1)), axis=1)\n",
        "\n",
        "  #d) retournez la séquence input_ids complétée.\n",
        "  return input_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfLCalxl4wM9"
      },
      "source": [
        "###e) Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNz9q_qh4y2n",
        "outputId": "58c64ba6-13a1-4dc8-b67e-d88d436b5eab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  10,   25, 6456, 6456, 6456, 6456, 6456, 6456, 6456, 6456, 6456,\n",
              "        6456, 6456, 6456, 6456, 6456, 6456, 6456, 6456, 6456, 6456, 6456]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(gptModel, np.array([[10,25]]), 20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
