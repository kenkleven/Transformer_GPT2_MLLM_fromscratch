# 1. Implémentation de l’Architecture Transformer avec NumPy
## Description :
Ce projet propose une implémentation complète de l’architecture Transformer, en se concentrant sur :
- Le mécanisme d’attention (Scaled Dot-Product Attention, Multi-Head Attention)
- Le bloc Encodeur

## Objectifs pédagogiques :
- Comprendre les étapes de traitement d’une séquence.
- Saisir le rôle du masquage, de la normalisation, et du feedforward.

## Technologies :
- Python 3
- NumPy uniquement (aucune bibliothèque de deep learning comme PyTorch ou TensorFlow)


# 2. Implémentation de l’Architecture GPT-2 avec NumPy
## Description :
Cette implémentation reproduit le cœur de GPT-2, un modèle de langage auto-régressif basé uniquement sur le décodeur Transformer.

## Fonctionnalités :
- Masquage causal dans l’attention

## Objectifs pédagogiques :
- Appréhender les modèles de génération de texte.
- Maîtriser les décodeurs Transformer appliqués à des tâches NLP.

## Technologies :
- Python 3
- NumPy


# 3. Construire l’Architecture d’un MLLM Audio-Texte
## Description :
Ce projet vise à concevoir une architecture multimodale capable de traiter de l’audio (forme d’onde) et de générer ou de comprendre du texte.

## Étapes couvertes :
- Extraction de caractéristiques audio
- Encodage audio
- Traitement textuel
- Fusion des modalités pour la synthèse ou la compréhension guidée par le texte


## Technologies :
- Python 3
- NumPy